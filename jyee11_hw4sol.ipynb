{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from ps4_utils import load_data,load_experiment\n",
    "from ps4_utils import AbstractGenerativeModel\n",
    "from ps4_utils import save_submission\n",
    "from scipy.misc import logsumexp\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "#data_fn = \"datasets-hw4.h5\"\n",
    "data_fn = \"datasets-hw4-1.h5\"\n",
    "\n",
    "MAX_OUTER_ITER = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MixtureModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, NUM_MIXTURE_COMPONENTS, MAX_ITER=50, EPS=10**(-7)):\n",
    "        AbstractGenerativeModel.__init__(self, CLASSES, NUM_FEATURES)\n",
    "        self.num_mixture_components = NUM_MIXTURE_COMPONENTS # list of num_mixture_components (length num_classes)\n",
    "        self.max_iter = MAX_ITER # max iterations of EM\n",
    "        self.epsilon = EPS # help with stability, to be used according to hint given at end of pset4.pdf\n",
    "        self.params = { # lists of length CLASSES\n",
    "            'pi': [np.repeat(1/k,k) for k in self.num_mixture_components], # with pi_c for each class\n",
    "            'theta': [np.zeros((self.num_features,k)) for k in self.num_mixture_components], # with theta_c for each class\n",
    "        }\n",
    "        \n",
    "    def pack_params(self, X, class_idx):\n",
    "        pi,theta = self.fit(X[class_idx],class_idx) # fit parameters\n",
    "        self.params['pi'][class_idx] = pi # update member variable pi\n",
    "        self.params['theta'][class_idx] = theta #update member variable theta\n",
    "        \n",
    "    #make classification based on which mixture model gives higher probability to generating point xi\n",
    "    def classify(self, X):\n",
    "        P = list()\n",
    "        pi = self.params['pi']\n",
    "        theta = self.params['theta']\n",
    "        for c in range(self.num_classes):\n",
    "            _,Pc = self.findP(X, pi[c], theta[c])\n",
    "            P.append(Pc)\n",
    "        return np.vstack(P).T.argmax(-1) # np.array of class predictions for each data point in X\n",
    "\n",
    "    # --- E-step\n",
    "    def updateLatentPosterior(self, X, pi, theta, num_mixture_components): # update the latent posterior\n",
    "        # YOUR CODE HERE\n",
    "        # --- gamma: responsibilities (probabilities), np.array (matrix)\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c]\n",
    "        # note: can use output of findP here (with care taken to return gamma containing proper probabilities)\n",
    "        #N = X.shape[0]\n",
    "        #gamma = np.ones(N,num_mixture_components) #theta.shape[1]\n",
    "        t,logsumt = self.findP(X,pi,theta)\n",
    "\n",
    "        T = np.repeat(logsumt, num_mixture_components).reshape((-1, num_mixture_components))\n",
    "        \"\"\"\n",
    "        T = np.ones((t.shape[0],t.shape[1]))\n",
    "        for i in range(t.shape[0]):\n",
    "            for j in range(t.shape[1]):\n",
    "                T[i][j] = logsumt[i]\n",
    "        \"\"\"\n",
    "        gamma = np.exp(t-T)\n",
    "           \n",
    "        return gamma\n",
    "    # --- M-step (1)\n",
    "    @staticmethod\n",
    "    def updatePi(gamma): #update the pi component using the posteriors (gammas)\n",
    "        # YOUR CODE HERE\n",
    "        # --- pi_c: class specific pi, np.array (vector)\n",
    "        # ---        shape: NUM_MIXTURE_COMPONENTS[c]\n",
    "\n",
    "        pi_c = np.sum(gamma, axis=0)/gamma.shape[0]\n",
    "        return pi_c\n",
    "    # -- M-step (2)\n",
    "    @staticmethod\n",
    "    def updateTheta(X, gamma): #update theta component using posteriors (gammas)\n",
    "        # YOUR CODE HERE\n",
    "        # --- theta_c: class specific theta, np.array matrix\n",
    "        # ---        shape: NUM_FEATURES by NUM_MIXTURE_COMPONENTS[c]\n",
    "        Nk = np.sum(gamma, axis=0)\n",
    "        matrix = np.dot(np.transpose(X), gamma)\n",
    "        theta_c = matrix/Nk\n",
    "        return theta_c \n",
    "    \n",
    "    @staticmethod\n",
    "    def findP(X, pi, theta):\n",
    "        # YOUR CODE HERE\n",
    "        # NOTE: you can also use t as a probability, just change \"logsumexp(t,axis=1)\" to \"logsumexp(np.log(t),axis=1)\"\n",
    "        # --- t: logprobabilities of x given each component of mixture\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c] \n",
    "        # --- logsumexp(t,axis=1): (for convenience) once exponentiated, gives normalization factor over all mixture components\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c)\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        t = np.ones([N,theta.shape[1]]) \n",
    "                    \n",
    "        \n",
    "        for c in range(pi.shape[0]):\n",
    "            pi_c = pi[c]\n",
    "            for i in range(0, N):\n",
    "                theta_cj = theta[:,c]\n",
    "                prob = np.log(pi_c)+ np.sum((np.log(theta_cj)*X[i,:]) + (np.log(1-theta_cj)*(1-X[i,:])))\n",
    "                t[i][c] = prob #np.log(pi_c)+\n",
    "        \"\"\"       \n",
    "        for c in range(pi.shape[0]):\n",
    "            pi_c = pi[c]\n",
    "            for i in range(0, N):\n",
    "                prob = 0\n",
    "                for j in range (0, d):\n",
    "                    theta_cj = theta[j][c]\n",
    "                    prob = prob + (np.log(theta_cj)*X[i][j]) + (np.log(1-theta_cj)*(1-X[i][j]))\n",
    "                t[i][c] = np.log(pi_c)+prob\n",
    "                \n",
    "        \n",
    "        for i in range(0, N):\n",
    "            for c in range(theta.shape[1]):\n",
    "                prob = 0\n",
    "                pi_c = pi[c]\n",
    "                for j in range (0, d):\n",
    "                    theta_cj = theta[j][c]\n",
    "                    prob = prob + (np.log(theta_cj)*X[i][j]) + (np.log(1-theta_cj)*(1-X[i][j]))\n",
    "                t[i][c] = np.log(pi_c)+prob \n",
    "        \"\"\"\n",
    "        return t,logsumexp(t,axis=1)\n",
    "        \n",
    "    # --- execute EM procedure\n",
    "    def fit(self, X, class_idx):\n",
    "        max_iter = self.max_iter\n",
    "        eps = self.epsilon\n",
    "        N = X.shape[0]\n",
    "        pi = self.params['pi'][class_idx]\n",
    "        theta = self.params['theta'][class_idx]\n",
    "        num_mixture_components = self.num_mixture_components[class_idx]\n",
    "        # INITIALIZE theta, note theta is currently set to zeros but needs to be officially initialized here\n",
    "        for i in range(num_mixture_components):\n",
    "            theta[:,i] = (np.sum(X[range(i,N,num_mixture_components),:], axis=0) + self.epsilon) / np.size(X[range(i,N,num_mixture_components),:],0)\n",
    "        \n",
    "        for i in range(theta.shape[0]):\n",
    "            for j in range(theta.shape[1]):   \n",
    "                if theta[i][j] < eps:\n",
    "                    theta[i][j] = eps\n",
    "                elif theta[i][j] > 1-eps:\n",
    "                    theta[i][j] = 1-eps\n",
    "                    \n",
    "        for i in range(max_iter):\n",
    "            # YOUR CODE HERE, E-step: \n",
    "            gamma = self.updateLatentPosterior(X, pi, theta, num_mixture_components)\n",
    "            # YOUR CODE HERE, M-step(1): \n",
    "            pi = self.updatePi(gamma)\n",
    "            # YOUR CODE HERE, M-step(2): \n",
    "            theta = self.updateTheta(X,gamma)\n",
    "            \n",
    "            for i in range(theta.shape[0]):\n",
    "                for j in range(theta.shape[1]):   \n",
    "                    if theta[i][j] < eps:\n",
    "                        theta[i][j] = eps\n",
    "                    elif theta[i][j] > 1-eps:\n",
    "                        theta[i][j] = 1-eps\n",
    "        \n",
    "        return pi,theta #pi and theta, given class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, EPS=10**(-12)):\n",
    "        AbstractGenerativeModel.__init__(self, CLASSES, NUM_FEATURES)\n",
    "        self.epsilon = EPS # help with stability\n",
    "        self.params = {\n",
    "            'p': [np.zeros((NUM_FEATURES))] * self.num_classes # estimated log-probabilities of features for each class\n",
    "        }\n",
    "    def pack_params(self, X, class_idx):\n",
    "        p = self.fit(X[class_idx])\n",
    "        self.params['p'][class_idx] = p\n",
    "    def classify(self, X): # naive bayes classifier\n",
    "        # YOUR CODE HERE\n",
    "        # --- predictions: predictions for data points in X (where X consists of datapoints from class c), np.array (vector)\n",
    "        # ---        shape: number of data points\n",
    "        N = X.shape[0]\n",
    "        d = X.shape[1]\n",
    "        predictions = np.zeros(N)\n",
    "        prob = np.ones((self.num_classes,N))\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            for i in range(0, N):\n",
    "                row = X[i]\n",
    "                for j in range (0, d):\n",
    "                    theta = self.params['p'][c][j]\n",
    "                    prob[c][i] = prob[c][i] * (theta**row[j])*((1-theta)**(1-row[j]))\n",
    "\n",
    "        predictions = prob.argmax(axis=0)\n",
    "        return predictions\n",
    "    def fit(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        # --- estimated_p: estimated p's of features for input X (where X consists of datapoints from class c), np.array (vector)\n",
    "        # ---        shape: NUM_FEATURES\n",
    "        estimated_p = np.zeros(X.shape[1])\n",
    "        sum_arr = np.sum(X, axis=0)\n",
    "        for i in range(0, X.shape[1]):\n",
    "            estimated_p[i] = (sum_arr[i]+self.epsilon)/X.shape[0]\n",
    "        return estimated_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.738\n",
      "CONFUSION MATRIX: \n",
      "[[  94.   60.]\n",
      " [  71.  275.]]\n",
      "SENTIMENT ANALYSIS -- MIXTURE MODEL:\n",
      "COMPONENTS: 2 4\n",
      "ACCURACY ON VALIDATION: 0.712\n",
      "COMPONENTS: 4 12\n",
      "ACCURACY ON VALIDATION: 0.704\n",
      "COMPONENTS: 13 13\n",
      "ACCURACY ON VALIDATION: 0.738\n",
      "COMPONENTS: 5 9\n",
      "ACCURACY ON VALIDATION: 0.714\n",
      "COMPONENTS: 6 12\n",
      "ACCURACY ON VALIDATION: 0.694\n",
      "COMPONENTS: 12 3\n",
      "ACCURACY ON VALIDATION: 0.696\n",
      "COMPONENTS: 14 9\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "COMPONENTS: 13 10\n",
      "ACCURACY ON VALIDATION: 0.716\n",
      "COMPONENTS: 7 13\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "COMPONENTS: 2 7\n",
      "ACCURACY ON VALIDATION: 0.696\n",
      "COMPONENTS: 10 3\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "COMPONENTS: 14 10\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "COMPONENTS: 11 7\n",
      "ACCURACY ON VALIDATION: 0.726\n",
      "COMPONENTS: 13 8\n",
      "ACCURACY ON VALIDATION: 0.716\n",
      "COMPONENTS: 7 2\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "('Saved:', 'mm-sentiment_analysis-submission.csv')\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"sentiment_analysis\"\n",
    "# --- SENTIMENT ANALYSIS setup\n",
    "Xtrain,Xval,num_classes,num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "print(\"CONFUSION MATRIX: \")\n",
    "cmatrix = np.zeros((num_classes, num_classes))\n",
    "for actual_c in range(num_classes):\n",
    "    classes = nbm.classify(Xval[actual_c]) \n",
    "    for predict_c in classes:\n",
    "        cmatrix[predict_c][actual_c] += 1\n",
    "print(cmatrix)\n",
    "\n",
    "# -- build mixture model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "\n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.7355\n",
      "CONFUSION MATRIX: \n",
      "[[ 151.    0.    5.    3.    2.    7.    2.    1.    2.    1.]\n",
      " [   0.  206.   14.    7.    3.   12.    8.   13.   18.    5.]\n",
      " [   5.    5.  147.   10.    5.    3.    6.    2.   13.    3.]\n",
      " [   5.    1.   11.  137.    0.   31.    2.    3.   20.   10.]\n",
      " [   0.    2.    6.    0.  147.    9.    9.   12.    5.   22.]\n",
      " [  10.    3.    2.    8.    4.   91.    9.    1.    8.    5.]\n",
      " [  11.    1.    9.    2.    2.    5.  177.    0.    3.    1.]\n",
      " [   1.    2.    3.    4.    6.    6.    1.  179.    0.   21.]\n",
      " [   2.    2.    9.    8.    3.    6.    2.    2.  122.    4.]\n",
      " [   2.    1.    4.    4.   21.    2.    1.   20.    5.  114.]]\n",
      "MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\n",
      "COMPONENTS: 2 3 7 13 13 8 14 6 5 4\n",
      "ACCURACY ON VALIDATION: 0.766\n",
      "COMPONENTS: 5 2 5 3 3 7 12 10 10 14\n",
      "ACCURACY ON VALIDATION: 0.7775\n",
      "COMPONENTS: 14 4 8 6 7 9 9 9 7 7\n",
      "ACCURACY ON VALIDATION: 0.7815\n",
      "COMPONENTS: 7 4 6 9 13 7 11 5 13 4\n",
      "ACCURACY ON VALIDATION: 0.784\n",
      "COMPONENTS: 2 4 11 3 3 11 5 5 13 13\n",
      "ACCURACY ON VALIDATION: 0.7685\n",
      "COMPONENTS: 3 12 10 14 10 11 4 9 2 10\n",
      "ACCURACY ON VALIDATION: 0.777\n",
      "COMPONENTS: 8 5 3 5 6 12 5 3 5 4\n",
      "ACCURACY ON VALIDATION: 0.781\n",
      "COMPONENTS: 5 14 14 9 7 8 13 10 9 7\n",
      "ACCURACY ON VALIDATION: 0.785\n",
      "COMPONENTS: 12 4 7 9 11 7 13 7 10 7\n",
      "ACCURACY ON VALIDATION: 0.778\n",
      "COMPONENTS: 2 7 13 13 4 14 11 12 8 7\n",
      "ACCURACY ON VALIDATION: 0.7705\n",
      "COMPONENTS: 8 8 5 7 13 12 10 5 2 14\n",
      "ACCURACY ON VALIDATION: 0.7795\n",
      "COMPONENTS: 4 7 6 4 7 8 10 3 13 3\n",
      "ACCURACY ON VALIDATION: 0.7775\n",
      "COMPONENTS: 10 7 6 9 10 3 7 13 7 9\n",
      "ACCURACY ON VALIDATION: 0.773\n",
      "COMPONENTS: 5 11 7 11 6 3 8 10 2 6\n",
      "ACCURACY ON VALIDATION: 0.7805\n",
      "COMPONENTS: 3 9 6 10 13 6 7 14 9 6\n",
      "ACCURACY ON VALIDATION: 0.77\n",
      "('Saved:', 'mm-mnist-submission.csv')\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"mnist\"\n",
    "# --- MNIST DIGIT CLASSIFICATION setup\n",
    "Xtrain,Xval,num_classes,num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "print(\"CONFUSION MATRIX: \")\n",
    "cmatrix = np.zeros((num_classes, num_classes))\n",
    "for actual_c in range(num_classes):\n",
    "    classes = nbm.classify(Xval[actual_c]) \n",
    "    for predict_c in classes:\n",
    "        cmatrix[predict_c][actual_c] += 1\n",
    "print(cmatrix)\n",
    "\n",
    "# -- build mixture model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "    \n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PROBLEM 1:\n",
    "x-axis = predicted\n",
    "y-axis = actual\n",
    "\n",
    "SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\n",
    "ACCURACY ON VALIDATION: 0.738\n",
    "CONFUSION MATRIX: \n",
    "[[  94.   60.]\n",
    " [  71.  275.]]\n",
    "\n",
    "MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\n",
    "ACCURACY ON VALIDATION: 0.7355\n",
    "CONFUSION MATRIX: \n",
    "[[ 151.    0.    5.    3.    2.    7.    2.    1.    2.    1.]\n",
    " [   0.  206.   14.    7.    3.   12.    8.   13.   18.    5.]\n",
    " [   5.    5.  147.   10.    5.    3.    6.    2.   13.    3.]\n",
    " [   5.    1.   11.  137.    0.   31.    2.    3.   20.   10.]\n",
    " [   0.    2.    6.    0.  147.    9.    9.   12.    5.   22.]\n",
    " [  10.    3.    2.    8.    4.   91.    9.    1.    8.    5.]\n",
    " [  11.    1.    9.    2.    2.    5.  177.    0.    3.    1.]\n",
    " [   1.    2.    3.    4.    6.    6.    1.  179.    0.   21.]\n",
    " [   2.    2.    9.    8.    3.    6.    2.    2.  122.    4.]\n",
    " [   2.    1.    4.    4.   21.    2.    1.   20.    5.  114.]]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
